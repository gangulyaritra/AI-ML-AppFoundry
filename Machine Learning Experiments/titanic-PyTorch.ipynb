{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Titanic - Machine Learning from Disaster.**\n",
        "\n",
        "Start here! Predict survival on the Titanic and get familiar with ML basics.\n",
        "\n",
        "> [**Kaggle Dataset**](https://www.kaggle.com/competitions/titanic/data)\n",
        "\n"
      ],
      "metadata": {
        "id": "OmH_eCHCbQIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVqFY_TgZ88n"
      },
      "outputs": [],
      "source": [
        "# Install Kaggle.\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Files Upload.\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "qTbnqGLgaUMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Kaggle Folder.\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# Copy the kaggle.json to the folder created.\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Permission for the json file to act.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "FaddL3YtaWMt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Download.\n",
        "!kaggle competitions download -c titanic"
      ],
      "metadata": {
        "id": "y_DkBdEGaYcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Dataset.\n",
        "!unzip titanic.zip"
      ],
      "metadata": {
        "id": "G6yK3uVeacT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Titanic Classification using PyTorch.**"
      ],
      "metadata": {
        "id": "EzXKQFHHcJMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Dataset.\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "sub = pd.read_csv(\"gender_submission.csv\")\n",
        "\n",
        "\n",
        "# Data Preprocessing.\n",
        "def data_preprocess(dataframe):\n",
        "    # Drop Unimportant Features.\n",
        "    dataframe.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\n",
        "\n",
        "    # Encode Categorical Features.\n",
        "    sex = pd.get_dummies(dataframe[\"Sex\"], drop_first=True)\n",
        "    embark = pd.get_dummies(dataframe[\"Embarked\"], drop_first=True)\n",
        "\n",
        "    dataframe = pd.concat([dataframe, sex, embark], axis=1)\n",
        "    dataframe.drop([\"Sex\", \"Embarked\"], axis=1, inplace=True)\n",
        "\n",
        "    # Handle Missing Values.\n",
        "    dataframe.fillna(dataframe.mean(), inplace=True)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "# Apply Data Preprocessing.\n",
        "train = data_preprocess(train)\n",
        "test = data_preprocess(test)\n",
        "\n",
        "# Split Dataset into Feature and Target Set.\n",
        "X = train.iloc[:, 1:].values\n",
        "y = train.iloc[:, 0].values\n",
        "\n",
        "# Split Dataset into Training and Validation Set.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Feature Scaling.\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# 1. Model Building (Binaryclass Classification).\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, num_classes=2):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        self.layer_1 = nn.Linear(input_size, 256)\n",
        "        self.layer_2 = nn.Linear(256, 128)\n",
        "        self.layer_3 = nn.Linear(128, 32)\n",
        "        self.layer_out = nn.Linear(32, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.batchnorm_1 = nn.BatchNorm1d(256)\n",
        "        self.batchnorm_2 = nn.BatchNorm1d(128)\n",
        "        self.batchnorm_3 = nn.BatchNorm1d(32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.batchnorm_1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer_2(x)\n",
        "        x = self.batchnorm_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.layer_3(x)\n",
        "        x = self.batchnorm_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.layer_out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "model = NeuralNetwork(input_size=X_train.shape[1])\n",
        "print(model)\n",
        "\n",
        "\n",
        "# 2. Model Hyperparameters.\n",
        "num_epochs = 20\n",
        "batch_size = 16\n",
        "train_batch_num = len(X_train) // batch_size\n",
        "test_batch_num = len(X_test) // batch_size\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# 3. Model Training Loop.\n",
        "train_epoch_loss = 0\n",
        "val_epoch_loss = 0\n",
        "val_loss_min = np.Inf\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "\n",
        "    for i in range(train_batch_num):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "\n",
        "        X_train_data = Variable(torch.FloatTensor(X_train[start:end]))\n",
        "        y_train_data = Variable(torch.LongTensor(y_train[start:end]))\n",
        "\n",
        "        # Forward Pass and Loss Calculation.\n",
        "        optimizer.zero_grad()  # Clear Gradient.\n",
        "        y_train_pred = model(X_train_data)\n",
        "        train_loss = criterion(y_train_pred, y_train_data)\n",
        "\n",
        "        # Backward Pass and Weight's Updation.\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, train_labels = torch.max(y_train_pred, 1)\n",
        "        train_num_right = np.sum(train_labels.data.numpy() == y_train[start:end])\n",
        "        train_epoch_loss += train_loss.item() * batch_size\n",
        "\n",
        "    train_epoch_loss = train_epoch_loss / len(X_train)\n",
        "\n",
        "    # torch.no_grad() tells PyTorch not to perform back-propagation, which reduces memory usage and speeds up computation.\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        for i in range(test_batch_num):\n",
        "            start = i * batch_size\n",
        "            end = start + batch_size\n",
        "\n",
        "            X_test_data = Variable(torch.FloatTensor(X_test[start:end]))\n",
        "            y_test_data = Variable(torch.LongTensor(y_test[start:end]))\n",
        "\n",
        "            # Forward Pass and Loss Calculation.\n",
        "            y_test_pred = model(X_test_data)\n",
        "            val_loss = criterion(y_test_pred, y_test_data)\n",
        "\n",
        "            _, val_labels = torch.max(y_test_pred, 1)\n",
        "            val_num_right = np.sum(val_labels.data.numpy() == y_test[start:end])\n",
        "            val_epoch_loss += val_loss.item() * batch_size\n",
        "\n",
        "    val_epoch_loss = val_epoch_loss / len(X_test)\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch+0:03}: | Train Loss: {train_epoch_loss:.3f} | Val Loss: {val_epoch_loss:.3f} | Train Accuracy: {train_num_right/len(y_train[start:end]):.3f} | Val Accuracy: {val_num_right/len(y_test[start:end]):.3f}\"\n",
        "        )\n",
        "        if val_epoch_loss <= val_loss_min:\n",
        "            print(\n",
        "                \"Validation loss decreased ({:3f} ===> {:3f}). Saving the model...\".format(\n",
        "                    val_loss_min, val_epoch_loss\n",
        "                )\n",
        "            )\n",
        "            torch.save(model.state_dict(), \"titanic_model.pt\")\n",
        "            val_loss_min = val_epoch_loss\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "print(\"Training Ended!\")\n",
        "\n",
        "\n",
        "# 4. Model Prediction.\n",
        "test = test.iloc[:, :].values\n",
        "test = Variable(torch.FloatTensor(test), requires_grad=False)\n",
        "\n",
        "# Load PyTorch Model.\n",
        "titanic_model = NeuralNetwork(input_size=X_test.shape[1])\n",
        "titanic_model.load_state_dict(torch.load(\"titanic_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(test)\n",
        "\n",
        "_, labels = torch.max(predictions, 1)\n",
        "survived = labels.data.numpy()\n",
        "\n",
        "# 5. Final Submission.\n",
        "submission = pd.DataFrame({\"PassengerId\": sub[\"PassengerId\"], \"Survived\": survived})\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "FvheQ5gNcT0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e77987-efc4-4f17-ef5c-e63bd8fea7dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layer_1): Linear(in_features=8, out_features=256, bias=True)\n",
            "  (layer_2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (layer_3): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (layer_out): Linear(in_features=32, out_features=2, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (batchnorm_1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm_3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "Epoch 001: | Train Loss: 0.503 | Val Loss: 0.478 | Train Accuracy: 0.750 | Val Accuracy: 0.812\n",
            "Validation loss decreased (inf ===> 0.477631). Saving the model...\n",
            "\n",
            "Epoch 002: | Train Loss: 0.453 | Val Loss: 0.454 | Train Accuracy: 0.750 | Val Accuracy: 0.812\n",
            "Validation loss decreased (0.477631 ===> 0.454448). Saving the model...\n",
            "\n",
            "Epoch 003: | Train Loss: 0.416 | Val Loss: 0.486 | Train Accuracy: 0.688 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 004: | Train Loss: 0.396 | Val Loss: 0.485 | Train Accuracy: 0.812 | Val Accuracy: 0.812\n",
            "\n",
            "Epoch 005: | Train Loss: 0.383 | Val Loss: 0.507 | Train Accuracy: 0.812 | Val Accuracy: 0.812\n",
            "\n",
            "Epoch 006: | Train Loss: 0.353 | Val Loss: 0.498 | Train Accuracy: 0.812 | Val Accuracy: 0.812\n",
            "\n",
            "Epoch 007: | Train Loss: 0.353 | Val Loss: 0.523 | Train Accuracy: 0.812 | Val Accuracy: 0.812\n",
            "\n",
            "Epoch 008: | Train Loss: 0.343 | Val Loss: 0.528 | Train Accuracy: 0.750 | Val Accuracy: 0.812\n",
            "\n",
            "Epoch 009: | Train Loss: 0.331 | Val Loss: 0.566 | Train Accuracy: 0.812 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 010: | Train Loss: 0.330 | Val Loss: 0.558 | Train Accuracy: 0.688 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 011: | Train Loss: 0.311 | Val Loss: 0.573 | Train Accuracy: 0.812 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 012: | Train Loss: 0.323 | Val Loss: 0.610 | Train Accuracy: 0.812 | Val Accuracy: 0.688\n",
            "\n",
            "Epoch 013: | Train Loss: 0.280 | Val Loss: 0.560 | Train Accuracy: 0.812 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 014: | Train Loss: 0.281 | Val Loss: 0.595 | Train Accuracy: 0.750 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 015: | Train Loss: 0.256 | Val Loss: 0.606 | Train Accuracy: 0.750 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 016: | Train Loss: 0.238 | Val Loss: 0.595 | Train Accuracy: 0.812 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 017: | Train Loss: 0.235 | Val Loss: 0.594 | Train Accuracy: 0.875 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 018: | Train Loss: 0.238 | Val Loss: 0.627 | Train Accuracy: 0.812 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 019: | Train Loss: 0.219 | Val Loss: 0.666 | Train Accuracy: 0.938 | Val Accuracy: 0.750\n",
            "\n",
            "Epoch 020: | Train Loss: 0.253 | Val Loss: 0.638 | Train Accuracy: 0.875 | Val Accuracy: 0.750\n",
            "\n",
            "Training Ended!\n"
          ]
        }
      ]
    }
  ]
}