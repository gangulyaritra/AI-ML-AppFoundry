{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Iz6Tm0yIra"
      },
      "source": [
        "# **Credit Card Fraud Detection using Cost-Sensitive Learning.**\n",
        "\n",
        "> [**Kaggle Dataset**](https://www.kaggle.com/mlg-ulb/creditcardfraud)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Kaggle.\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "BbsoZkvj-S00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Files Upload.\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "utO0nRwG-S3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Kaggle Folder.\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# Copy the kaggle.json to the folder created.\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Permission for the json file to act.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "mCngzL88-S56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Download.\n",
        "!kaggle datasets download -d mlg-ulb/creditcardfraud"
      ],
      "metadata": {
        "id": "fsN6BWsa-S8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip Dataset.\n",
        "!unzip creditcardfraud.zip"
      ],
      "metadata": {
        "id": "mCyNb87f-S-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGd1gZjduubB"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8p7-EBbfGmK"
      },
      "source": [
        "# Import Library.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras import initializers, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow_addons.metrics import CohenKappa\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Import Dataset.\n",
        "data = pd.read_csv(\"creditcard.csv\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUVOLLKKjeTn"
      },
      "source": [
        "# Class Distribution of Dataset.\n",
        "print(data[\"Class\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVc84YTjpVvU"
      },
      "source": [
        "# Visualize the Class Distribution.\n",
        "pd.value_counts(data[\"Class\"]).plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZHx5snlkR0o"
      },
      "source": [
        "# Split Dataset into Feature and Target Set.\n",
        "X = data.iloc[:, 1:30]\n",
        "y = data.iloc[:, -1]\n",
        "\n",
        "# Split Dataset into Training and Test Set.\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Model Configuration.\n",
        "BATCH_SIZE = 256\n",
        "NO_EPOCHS = 100\n",
        "NO_CLASSES = 2\n",
        "VALIDATION_SPLIT = 0.2\n",
        "VERBOSITY = 1\n",
        "my_callbacks = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=20, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "\n",
        "# Model Architecture/Pipeline.\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(\n",
        "        Dense(\n",
        "            units=100,\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2=0.01),\n",
        "        )\n",
        "    )\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(\n",
        "        Dense(\n",
        "            units=100,\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2=0.01),\n",
        "        )\n",
        "    )\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(\n",
        "        Dense(\n",
        "            units=100,\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2=0.01),\n",
        "        )\n",
        "    )\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(\n",
        "        Dense(\n",
        "            units=100,\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2=0.01),\n",
        "        )\n",
        "    )\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(\n",
        "        Dense(\n",
        "            units=100,\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2=0.01),\n",
        "        )\n",
        "    )\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile the Model.\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\", CohenKappa(num_classes=NO_CLASSES)],\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbBtJ4TWlMnV"
      },
      "source": [
        "# Call the Model Architecture.\n",
        "model = create_model()\n",
        "\n",
        "# Build the Model.\n",
        "model.build(X_train.shape)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu_ziaM-nPm-"
      },
      "source": [
        "\"\"\"\n",
        "Weighted Neural Network:\n",
        "    1. Define Weights.\n",
        "    2. Fit the model with those specific weights.\n",
        "\"\"\"\n",
        "\n",
        "# The class distribution of the \"creditcard.csv\" dataset has a 1:560 ratio for the minority class to the majority class.\n",
        "weights_assigned = {0: 1, 1: 560}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiH-QKM8nPnt"
      },
      "source": [
        "# Fit the Model.\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    class_weight=weights_assigned,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=NO_EPOCHS,\n",
        "    verbose=VERBOSITY,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    callbacks=my_callbacks,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ni0cphRnYil"
      },
      "source": [
        "# Plot Training and Validation Graph.\n",
        "kap = history.history[\"cohen_kappa\"]\n",
        "val_kappa = history.history[\"val_cohen_kappa\"]\n",
        "epochs = len(kap)\n",
        "plt.plot(np.arange(epochs), kap, label=\"Training Kappa\")\n",
        "plt.plot(np.arange(epochs), val_kappa, \"g\", label=\"Validation Kappa\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3ST6XbongUm"
      },
      "source": [
        "# Model Evaluation.\n",
        "print(model.evaluate(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ80BnuUoSdP",
        "outputId": "fb4b76b8-248e-48f4-cd2d-6aae0bf602d7"
      },
      "source": [
        "# Performance Score.\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"\\n ROC-AUC Score is \", roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2226/2226 [==============================] - 4s 2ms/step\n",
            "\n",
            " ROC-AUC Score is  0.9507912700365344\n"
          ]
        }
      ]
    }
  ]
}